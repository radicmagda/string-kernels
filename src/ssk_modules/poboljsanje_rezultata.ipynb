{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175922.15625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from main import sskpy\n",
    "\n",
    "prvi = \"This is a very long string, just to test how fast this implementation \" \\\n",
    "        \"oook like ttime, unless you're\" \\\n",
    "        \" running this in a potato pc\"\n",
    "drugi=\"dfgbs sfhsbgs vsjh\"\n",
    "a=0.8\n",
    "rez=sskpy(prvi, prvi, 9,a )\n",
    "print(rez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary computing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(first_dataset, second_dataset, similarity_function, symmetrical=False):\n",
    "    \"\"\"\n",
    "    Calculate the similarity matrix between elements of two datasets using a similarity function.\n",
    "\n",
    "    Args:\n",
    "    - first_dataset: List or array-like, the first dataset\n",
    "    - second_dataset: List or array-like, the second dataset\n",
    "    - similarity_function: Function, the similarity function that takes two elements as arguments\n",
    "\n",
    "    Returns:\n",
    "    - similarity_matrix: NumPy ndarray, the similarity matrix\n",
    "    \"\"\"\n",
    "    if(symmetrical):\n",
    "        size = len(first_dataset)\n",
    "        similarity_matrix = [[0.0] * size for _ in range(size)]\n",
    "\n",
    "        for i in range(size):\n",
    "            for j in range(i, size):\n",
    "                value =similarity_function(first_dataset[i], second_dataset[j])\n",
    "                similarity_matrix[i][j] = value\n",
    "                similarity_matrix[j][i] = value \n",
    "    else:\n",
    "        similarity_matrix = [[similarity_function(x, y) for y in second_dataset] for x in first_dataset]\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import sskpy\n",
    "def ssk(a, b, k, lambd):\n",
    "    return sskpy(a,b,k,lambd)\n",
    "\n",
    "def ssk_scaled(a, b, k, lambd):    #accounts for document length\n",
    "    len_a=len(a)\n",
    "    len_b=len(b)\n",
    "    return sskpy(a,b,k,lambd)/(max(len_a, len_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssk_partial(ka, Lambda):\n",
    "    return lambda a,b: ssk(a,b,k=ka, lambd=Lambda)\n",
    "\n",
    "def ssk_scaled_partial(ka, Lambda):\n",
    "    return lambda a,b: ssk_scaled(a,b,k=ka, lambd=Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel runs definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def WK_SVM(X_train, y_train, X_test, y_test):  #a multi-class classifier\n",
    "\n",
    "    '''calculates f1, precision, and recall for a SVM classifer using linear tfidf mapping\n",
    "    Args:\n",
    "    - X_train,y_train, X_test, y_test\n",
    "    Returns:\n",
    "    - f1, precision, recall: for each of the classes in form of a pandas dataframe w columns Kernel, Class, F1, Precision , Recall\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, use_idf=True, smooth_idf=True, norm='l2',\n",
    "                                 analyzer='word', stop_words='english')\n",
    "\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    svm_classifier = SVC(kernel='linear')\n",
    "    svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "    results_df = pd.DataFrame({\n",
    "    'Kernel' : 'WK',\n",
    "    'Class': range(len(precision)),\n",
    "    'F1-Score': f1,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    })\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def NGK_SVM(X_train, y_train, X_test, y_test, k):\n",
    "    '''calculates f1, precision, and recall for a SVM classifer using linear n-gram mapping\n",
    "    Args:\n",
    "    - X_train,y_train, X_test, y_test, k=lenthg of the ngrams\n",
    "    Returns:\n",
    "    - f1, precision, recall: for each of the classes in form of a pandas dataframe w columns Kernel,k, Class, F1, Precision , Recall\n",
    "    '''\n",
    "    ngram_range = (k, k)\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=ngram_range)\n",
    "\n",
    "    x_train_ngrams = normalize(vectorizer.fit_transform(X_train), norm='l2')  #ngram vectors normalised to l2 norm\n",
    "    x_test_ngrams = normalize(vectorizer.transform(X_test), norm='l2')\n",
    "\n",
    "    svm_classifier = SVC(kernel='linear')\n",
    "    svm_classifier.fit(x_train_ngrams, y_train)\n",
    "\n",
    "    y_pred = svm_classifier.predict(x_test_ngrams)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Kernel':'NGK',\n",
    "        'k':k,\n",
    "        'Class': range(len(precision)),\n",
    "        'F1-Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "        })\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSK_SCALED_SVM(X_train, y_train, X_test, y_test, k, lambd ):\n",
    "    '''calculates f1, precision, and recall for a SVM classifer using SSK\n",
    "    Args:\n",
    "    - X_train,y_train, X_test, y_test, k=lenthg of the substrings, lambd= weight decay factor\n",
    "    Returns:\n",
    "    - f1, precision, recall: for each of the classes in form of a pandas dataframe w columns Kernel,k,lambd, Class, F1, Precision , Recall\n",
    "    '''\n",
    "    # calculate gram matrix for training and matrix for prediction\n",
    "    kernel_function=ssk_scaled_partial(k, lambd)\n",
    "    train_matrix=similarity_matrix(X_train, X_train, kernel_function, symmetrical=True)\n",
    "    test_matrix=similarity_matrix(X_test, X_train, kernel_function)\n",
    "\n",
    "    #model - precomputed, trained on the gram matrix\n",
    "    svm_model=SVC(kernel='precomputed')\n",
    "    svm_model.fit(train_matrix, y_train)\n",
    "\n",
    "    #predicting\n",
    "    y_pred=svm_model.predict(test_matrix)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Kernel':'SSK',\n",
    "        'k':k,\n",
    "        'lambda': lambd,\n",
    "        'Class': range(len(precision)),\n",
    "        'F1-Score': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall\n",
    "        })\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating datasets\n",
    "- using 4 classes - 100 examples from each class = total dataset of 400 labeled documents\n",
    "- splitting it into 5 train/test pairs in 80/20 ratio using StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('../data/preprocessed.csv')[['topics', 'body']].copy()\n",
    "topic_mapping = {'earn': 0, 'acq': 1, 'crude': 2, 'grain': 3}\n",
    "data['topics'] = data['topics'].map(topic_mapping)\n",
    "\n",
    "N_classes=4\n",
    "n_per_class=100\n",
    "\n",
    "class0df=data[data.topics==0].head(n_per_class)  #earn\n",
    "class1df=data[data.topics==1].head(n_per_class)  #acq\n",
    "class2df=data[data.topics==2].head(n_per_class)  #crude\n",
    "class3df=data[data.topics==3].head(n_per_class)  #grain\n",
    "\n",
    "final=pd.concat([class0df, class1df, class2df, class3df][:N_classes])\n",
    "print(len(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X_all=np.array(final['body'])\n",
    "y_all=np.array(final['topics'])\n",
    "\n",
    "n_folds = 5\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "datasets = []\n",
    "\n",
    "for train_index, test_index in stratified_kfold.split(X_all,y_all):\n",
    "    X_train, X_test = X_all[train_index], X_all[test_index]\n",
    "    y_train, y_test = y_all[train_index], y_all[test_index]\n",
    "    \n",
    "    datasets.append((X_train, X_test, y_train, y_test))\n",
    "\n",
    "# datasets[i] = Xtrain_ i , Xtest_i, ytrain_i, ytest_i, i=1,...,10 aka the index of the run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeating experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 \n",
    "- varying k but using the scaled version of the kernel function\n",
    "- keeping lambda at 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_default=0.5\n",
    "kvalues=[3,4,5,6,7,8,10,12]\n",
    "\n",
    "experiment_results=[]\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    Xtrain, Xtest, ytrain, ytest= datasets[i]\n",
    "\n",
    "    wk_results=WK_SVM(Xtrain,ytrain, Xtest,ytest)\n",
    "    experiment_results.append(wk_results)\n",
    "\n",
    "    for k in kvalues:\n",
    "        ngk_results=NGK_SVM(Xtrain,ytrain, Xtest,ytest,k)\n",
    "        ssk_results=SSK_SCALED_SVM(Xtrain,ytrain, Xtest,ytest,k, lambda_default)\n",
    "        experiment_results.append(ngk_results)\n",
    "        experiment_results.append(ssk_results)\n",
    "\n",
    "results1=pd.concat(experiment_results)\n",
    "\n",
    "\n",
    "csv_filename=f\"exp1_scaledssk_{n_folds}_iterations__{N_classes}_classes__{n_per_class}_in_each_class.csv\"\n",
    "results1.to_csv(f\"../data/results/scaled_ssk/{csv_filename}\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 \n",
    "- varying lambda while keeping k constant at 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_default=5\n",
    "lambdavalues=[0.01, 0.03, 0.05, 0.07, 0.09, 0.1, 0.3, 0.5, 0.7]\n",
    "\n",
    "exp2_results=[]\n",
    "\n",
    "for i_exp2 in range(len(datasets)):\n",
    "    Xtrainexp2, Xtestexp2, ytrainexp2, ytestexp2= datasets[i_exp2]\n",
    "\n",
    "    wke2_results=WK_SVM(Xtrainexp2,ytrainexp2, Xtestexp2,ytestexp2)\n",
    "    exp2_results.append(wke2_results)\n",
    "\n",
    "    ngke2_results=NGK_SVM(Xtrainexp2,ytrainexp2, Xtestexp2,ytestexp2,k_default)\n",
    "    exp2_results.append(ngke2_results)\n",
    "\n",
    "    for l in lambdavalues:\n",
    "        sske2_results=SSK_SCALED_SVM(Xtrainexp2,ytrainexp2, Xtestexp2,ytestexp2,k_default, l)\n",
    "        exp2_results.append(sske2_results)\n",
    "\n",
    "\n",
    "results2=pd.concat(exp2_results)\n",
    "\n",
    "\n",
    "csv2_filename=f\"exp2_scaledssk_{n_folds}_iterations__{N_classes}_classes__{n_per_class}_in_each_class.csv\"\n",
    "results2.to_csv(f\"../data/results/scaled_ssk/{csv2_filename}\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
