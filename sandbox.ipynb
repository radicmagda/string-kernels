{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\magda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "                   topics                                          body\n",
      "0       smart-good-school  This is a sample text with some punctuation!\n",
      "1  funny-interesting-home     Another example with interesting content.\n",
      "2             smart-funny                       A funny and smart text.\n",
      "3               good-home                          A good home for all.\n",
      "4       smart-good-school                          Smart, good, school.\n",
      "\n",
      "DataFrame with Processed 'body' Column:\n",
      "                   topics                       body_processed\n",
      "0       smart-good-school              sample text punctuation\n",
      "1  funny-interesting-home  another example interesting content\n",
      "2             smart-funny                     funny smart text\n",
      "3               good-home                            good home\n",
      "4       smart-good-school                    smart good school\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'topics': ['smart-good-school', 'funny-interesting-home', 'smart-funny', 'good-home', 'smart-good-school'],\n",
    "        'body': ['This is a sample text with some punctuation!',\n",
    "                 'Another example with interesting content.',\n",
    "                 'A funny and smart text.',\n",
    "                 'A good home for all.',\n",
    "                 'Smart, good, school.']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    '''replaces punctuation and stopwords with spaces, turns everything to lowercase, removes possible multiple spaces'''\n",
    "    # replace all punctuation with a space\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # tokenize the text\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase for consistency\n",
    "    \n",
    "    # replace all stop words with space\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [' ' if word in stop_words else word for word in tokens]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    # replace multiple contiguous spaces with a single space\n",
    "    processed_text = ' '.join(processed_text.split())\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply the preprocessing function to the 'body' column\n",
    "df['body_processed'] = df['body'].apply(preprocess_text)\n",
    "\n",
    "# Display the result\n",
    "print(\"Original DataFrame:\")\n",
    "print(df[['topics', 'body']])\n",
    "\n",
    "print(\"\\nDataFrame with Processed 'body' Column:\")\n",
    "print(df[['topics', 'body_processed']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Kernel  Class  F1-Score  Precision    Recall\n",
      "0     WK      0  0.400000   0.500000  0.333333\n",
      "1     WK      1  0.444444   0.400000  0.500000\n",
      "2     WK      2  0.500000   0.500000  0.500000\n",
      "3     WK      3  0.666667   0.666667  0.666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "y_true = [0, 1, 2, 0, 1, 2, 3, 3,2,1,0,2,3,1]\n",
    "y_pred = [0, 1, 2, 1, 2, 0, 3, 3,1,2,3,2,1,1]\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Kernel' : 'WK',\n",
    "    'Class': range(len(precision)),\n",
    "    'F1-Score': f1,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    })\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         el        wo       d w      ell       hel       ld       llo  \\\n",
      "0  0.288675  0.000000  0.000000  0.57735  0.288675  0.000000  0.57735   \n",
      "1  0.000000  0.333333  0.333333  0.00000  0.000000  0.333333  0.00000   \n",
      "\n",
      "        lo        o e       orl       rld       wor  \n",
      "0  0.288675  0.288675  0.000000  0.000000  0.000000  \n",
      "1  0.000000  0.000000  0.333333  0.333333  0.666667  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "texts = [\"hello ello\", \"world wor\"]\n",
    "\n",
    "# Initialize CountVectorizer with character trigrams\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3))\n",
    "\n",
    "# Fit and transform the texts\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_norm=normalize(X, norm='l2')\n",
    "\n",
    "# Get the feature names (trigrams in this case)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better visualization\n",
    "X_dense = X_norm.toarray()\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X_dense, columns=feature_names)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ssk_modules.main' has no attribute 'sskpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20940/231274026.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdrugi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dfgbs sfhsbgs vsjh\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mrez\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msskpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprvi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrugi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ssk_modules.main' has no attribute 'sskpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ssk_modules.main as mk\n",
    "\n",
    "   \n",
    "prvi = \"This is a very long string, just to test how fast this implementation \" \\\n",
    "        \"of ssk is. It should look like the computation tooks no time, unless you're\" \\\n",
    "        \" running this in a potato pc\"\n",
    "drugi=\"dfgbs sfhsbgs vsjh\"\n",
    "a=0.8\n",
    "rez=mk.sskpy(prvi, drugi, 30,a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df is your DataFrame with 'topics' and 'body' columns\n",
    "# Replace this with your actual DataFrame\n",
    "\n",
    "# For the purpose of this example, let's create a sample DataFrame\n",
    "data = {'topics': [0, 1, 2, 3] * 48, 'body': range(192)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the desired number of records for each category in the training set\n",
    "train_counts = {'0': 152, '1': 114, '2': 76, '3': 38}\n",
    "\n",
    "# Define the desired number of records for each category in the testing set\n",
    "test_counts = {'0': 40, '1': 25, '2': 15, '3': 10}\n",
    "\n",
    "# Create training set\n",
    "train_set = df.groupby('topics', group_keys=False).apply(lambda x: x.sample(train_counts[str(x.name)]))\n",
    "\n",
    "# Create testing set\n",
    "test_set = df.drop(train_set.index).groupby('topics', group_keys=False).apply(lambda x: x.sample(test_counts[str(x.name)]))\n",
    "\n",
    "# Display the shapes of the resulting DataFrames\n",
    "print(\"Train set shape:\", train_set.shape)\n",
    "print(\"Test set shape:\", test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyximport; pyximport.install()  # noqa: E702\n",
    "\n",
    "from string_kernel import ssk, string_kernel\n",
    "import numpy as np\n",
    "\n",
    "test = \"This is a very long string, just to test how fast this implementation \" \\\n",
    "        \"of ssk is. It should look like the computation tooks no time, unless you're\" \\\n",
    "        \" running this in a potato pc\"\n",
    "print(ssk(test, test, 30, .8, accum=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'string_kernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23804/2492591776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mssk_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msskpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\magda\\Desktop\\diplomski-1.sem\\uuzop\\uuzop_projekt\\ssk_modules\\main.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyximport\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mpyximport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa: E702\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstring_kernel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mssk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_kernel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'string_kernel'"
     ]
    }
   ],
   "source": [
    "from ssk_modules.main import sskpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3761\n",
      "1    2186\n",
      "3     489\n",
      "2     483\n",
      "Name: topics, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('data/preprocessed.csv')[['topics', 'body']].copy()\n",
    "topic_mapping = {'earn': 0, 'acq': 1, 'crude': 2, 'grain': 3}\n",
    "data['topics'] = data['topics'].map(topic_mapping)\n",
    "\n",
    "topcc=data['topics'].value_counts()\n",
    "print(topcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class0df=data[data.topics==0].head(200)  #earn\n",
    "class1df=data[data.topics==1].head(200)  #acq\n",
    "class2df=data[data.topics==2].head(200)  #crude\n",
    "class3df=data[data.topics==3].head(200)  #grain\n",
    "\n",
    "final=pd.concat([class0df, class1df, class2df, class3df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 5\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "datasets = []\n",
    "\n",
    "import numpy as np\n",
    "N=5\n",
    "clas0minidf=data[data.topics==0].head(5)\n",
    "clas1minidf=data[data.topics==1].head(5)\n",
    "minidf=pd.concat([clas0minidf, clas1minidf])\n",
    "\n",
    "Xmini=np.array(minidf['body']).reshape(10,1)\n",
    "ymini=np.array(minidf['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def WK_SVM(X_train, y_train, X_test, y_test):  #a multi-class classifier\n",
    "\n",
    "    '''calculates f1, precision, and recall for a SVM classifer using linear tfidf mapping\n",
    "    Args:\n",
    "    - X_train,y_train, X_test, y_test\n",
    "    Returns:\n",
    "    - f1, precision, recall: for each of the classes in form of a pandas dataframe w columns Kernel, Class, F1, Precision , Recall\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, use_idf=True, smooth_idf=True, norm='l2',\n",
    "                                 analyzer='word', stop_words='english')\n",
    "\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train.flatten())\n",
    "    X_test_tfidf = vectorizer.transform(X_test.flatten())\n",
    "\n",
    "    svm_classifier = SVC(kernel='linear')\n",
    "    svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    y_pred = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "    results_df = pd.DataFrame({\n",
    "    'Kernel' : 'WK',\n",
    "    'Class': range(len(precision)),\n",
    "    'F1-Score': f1,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    })\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23804/3975676372.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mX_test_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mWK_SVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23804/3132418863.py\u001b[0m in \u001b[0;36mWK_SVM\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     16\u001b[0m                                  analyzer='word', stop_words='english')\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mX_train_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mX_test_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2075\u001b[0m         \"\"\"\n\u001b[0;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\magda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \"\"\"\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Use the split method to iterate over the folds\n",
    "datasets=[]\n",
    "for train_index, test_index in stratified_kfold.split(Xmini,ymini):\n",
    "    X_train, X_test = Xmini[train_index], Xmini[test_index]\n",
    "    y_train, y_test = ymini[train_index], ymini[test_index]\n",
    "    \n",
    "    datasets.append((X_train, X_test, y_train, y_test))\n",
    "\n",
    "#run 0\n",
    "    X_train0, X_test0, y_train0, y_test0=datasets[0]\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, use_idf=True, smooth_idf=True, norm='l2',\n",
    "                                 analyzer='word', stop_words='english')\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train0.flatten())\n",
    "X_test_tfidf = vectorizer.transform(X_test0.flatten())\n",
    "\n",
    "res=WK_SVM(X_train0, X_test0, y_train0, y_test0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
